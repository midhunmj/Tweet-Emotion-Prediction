{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyFXHeF8Fdtq"
   },
   "source": [
    "This project deals with the statistical approach - both conventional machine learning and deep learning techniques to identity which one of four emotions a tweet represents: anger, fear, joy, or sadness. \n",
    "\n",
    "The datasets used for this project is taken from the available modified datasets given in kaggle competition and some additional datasets from - [Affect in Tweets task at SemEval in 2018](https://competitions.codalab.org/competitions/17751#learn_the_details)\n",
    "\n",
    "The **Support Vector Classification** Model is the conventional machine learning model chosen here. Support Vector Classifiers are one of the most memory efficient classifier and works relatively well when there is a clear margin of classifications or separation between the classes. This is because SVM takes data as an input and outputs the line that separates the classes. \n",
    "\n",
    "The SVM consistently achieved good performance for emotion prediction outperforming other conventional models. With their ability to generalize well in high dimensional feature spaces, SVMs eliminate the need for feature selection, making the application of text categorization considerably easier.\n",
    "\n",
    "> TfidfVectorizer was used to transform text to feature vectors that can be used as input to estimator.\n",
    "\n",
    "The **Convolutional Neural Network** Model was the selected deep learning model here. Text as a sequence is passed to a CNN. The input is primarily passed to an embedding layer and then to a convolutional layer and a pooling layer respectively. Finally, a Dense layer with four classes and a sigmoid activation is applied.\n",
    "\n",
    "> Tokenizer utility class is used to vectorize a text corpus into a list of integers.\n",
    "\n",
    "> To counter the issue of text sequences with different length of words, pad_sequence() is used which simply pads the sequence of words with zeros.\n",
    "\n",
    "The convolutional Neural network was the best performing model among the two, which rendered an accuracy of **71.751%** for the public dataset while Support Vector Classification gave a slightly lower accuracy of **70.300 %**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "Bj078Nu2XmEO",
    "outputId": "0e9c0def-03ea-477d-a244-8f497e0fb002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#importing required libraries and modules\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.corpus import wordnet as part_of_speech\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection , naive_bayes , svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import metrics\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7hXkjNbbaTJA"
   },
   "source": [
    "The dataset consists of three types of files related to training data, validation data and public test data; all  in npy and xlsx format. One type of file contains the content of the tweets, with words represented by indexes; the other type of file contains the labels. There is also one single file mapping words to indexes, in Python pickle format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LpbmFXEpX_B6"
   },
   "outputs": [],
   "source": [
    "#first set of train data\n",
    "train_labels_main = np.load('text_train_labels.npy')\n",
    "with open('text_word_to_idx.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)\n",
    "train_tweets_main = np.load('text_train_tweets.npy')\n",
    "\n",
    "#second set of train data\n",
    "df_train_tweet2_main = pd.read_excel ('Trail Tweets.xlsx')\n",
    "df_train_label2_main  = pd.read_excel ('Trial labels.xlsx', header = None)\n",
    "\n",
    "#validation set\n",
    "Val_labels_main = np.load('text_val_labels.npy')\n",
    "Val_tweets_main = np.load('text_val_tweets.npy')\n",
    "\n",
    "#public tweets\n",
    "public_tweets = np.load('text_test_public_tweets_rand.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfCD5ti9Cjwh"
   },
   "source": [
    "The code below handles the data preprocessing and then feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Som0kb7XYVJw"
   },
   "outputs": [],
   "source": [
    "#Interchanging dictionary key and values\n",
    "new_dict = dict([(value, key) for key, value in data.items()])\n",
    "\n",
    "#decoding the tweets in train_tweets\n",
    "xyz = []\n",
    "for i in train_tweets_main:\n",
    "    new_data = []\n",
    "    for j in i:\n",
    "         new_data.append(new_dict[j] )\n",
    "    xyz.append(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAmUsRG6YXRt"
   },
   "outputs": [],
   "source": [
    "#part of speech tagging \n",
    "\n",
    "part_of_speech_tag = defaultdict (lambda : part_of_speech.NOUN)\n",
    "part_of_speech_tag['J'] = part_of_speech.ADJ\n",
    "part_of_speech_tag['V'] = part_of_speech.VERB\n",
    "part_of_speech_tag['R'] = part_of_speech.ADV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3-GctLX_GEp"
   },
   "source": [
    "Function to preprocess .npy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Vp6IBxqYa6e"
   },
   "outputs": [],
   "source": [
    "def preprocessinng(x):\n",
    "    xyz = []\n",
    "    for i in x:\n",
    "        new_data = []\n",
    "        for j in i:\n",
    "             new_data.append(new_dict[j] )\n",
    "        xyz.append(new_data)\n",
    "\n",
    "    df = pd.DataFrame(xyz)\n",
    "\n",
    "    #Some initial pre-processing\n",
    "    df.replace('<START>' , np.nan , inplace= True)\n",
    "    df.replace('<NULL>' , np.nan , inplace= True)\n",
    "    df.replace('<END>' , np.nan , inplace= True)\n",
    "    df.replace('<user>' , np.nan , inplace= True)\n",
    "    df['ColumnA'] = df[df.columns[:]].apply( lambda x: ','.join(x.dropna().astype(str)),axis=1)\n",
    "    df2 = pd.DataFrame(df['ColumnA']) \n",
    "    df2.replace('#' , \"\", inplace= True, regex = True)\n",
    "    #df2.replace('_' ,\" \" , inplace= True, regex = True)\n",
    "    df2['ColumnA'] = [entry.lower () for entry in df2['ColumnA']]\n",
    "    df2.replace('angry','anger',inplace=True, regex = True)\n",
    "    df2.replace('furious','anger',inplace=True, regex = True)\n",
    "    df2.replace('irritated','anger',inplace=True, regex = True)\n",
    "    df2.replace('enraged','anger',inplace=True, regex = True)\n",
    "    df2.replace('annoyed','anger',inplace=True, regex = True)\n",
    "    df2.replace('sad','sadness',inplace=True, regex = True)\n",
    "    df2.replace('depressed','sadness',inplace=True, regex = True)\n",
    "    df2.replace('devastated','sadness',inplace=True, regex = True)\n",
    "    df2.replace('miserable','sadness',inplace=True, regex = True)\n",
    "    df2.replace('disappointed','sadness',inplace=True, regex = True)\n",
    "    df2.replace('terrified','fear',inplace=True, regex = True)\n",
    "    df2.replace('discouraged','fear',inplace=True, regex = True)\n",
    "    df2.replace('scared','fear',inplace=True, regex = True)\n",
    "    df2.replace('anxious','fear',inplace=True, regex = True)\n",
    "    df2.replace('fearful','fear',inplace=True, regex = True)\n",
    "    df2.replace('happy','joy',inplace=True, regex = True)\n",
    "    df2.replace('ecstatic','joy',inplace=True, regex = True)\n",
    "    df2.replace('glad','joy',inplace=True, regex = True)\n",
    "    df2.replace('relieved','joy',inplace=True, regex = True)\n",
    "    df2.replace('excited','joy',inplace=True, regex = True)\n",
    "    df2.replace('irritating','anger',inplace=True, regex = True)\n",
    "    df2.replace('vexing','anger',inplace=True, regex = True)\n",
    "    df2.replace('outrageous','anger',inplace=True, regex = True)\n",
    "    df2.replace('annoying','anger',inplace=True, regex = True)\n",
    "    df2.replace('displeasing','sadness',inplace=True, regex = True)\n",
    "    df2.replace('depressing','sadness',inplace=True, regex = True)\n",
    "    df2.replace('serious','sadness',inplace=True, regex = True)\n",
    "    df2.replace('grim','sadness',inplace=True, regex = True)\n",
    "    df2.replace('heartbreaking','sadness',inplace=True, regex = True)\n",
    "    df2.replace('gloomy','fear',inplace=True, regex = True)\n",
    "    df2.replace('horrible','fear',inplace=True, regex = True)\n",
    "    df2.replace('threatening','fear',inplace=True, regex = True)\n",
    "    df2.replace('terrifying','fear',inplace=True, regex = True)\n",
    "    df2.replace('shocking','fear',inplace=True, regex = True)\n",
    "    df2.replace('dreadful','joy',inplace=True, regex = True)\n",
    "    df2.replace('funny','joy',inplace=True, regex = True)\n",
    "    df2.replace('hilarious','joy',inplace=True, regex = True)\n",
    "    df2.replace('amazing','joy',inplace=True, regex = True)\n",
    "    df2.replace('wonderful','joy',inplace=True, regex = True)\n",
    "    \n",
    "    # Remove blank rows.\n",
    "    df2['ColumnA'].dropna(inplace = True)\n",
    "\n",
    "    # Convert the text to lowercase\n",
    "    df2['ColumnA'] = [entry.lower () for entry in df2['ColumnA']]\n",
    "\n",
    "    # Tokenise the text \n",
    "    df2['ColumnA']= [word_tokenize (entry) for entry in df2['ColumnA']]\n",
    "\n",
    "    # Remove stop words and non alphabetic words and perform lemmatisation\n",
    "    for index, entry in enumerate(df2['ColumnA']):\n",
    "        final_words= []\n",
    "        word_lemmatized= WordNetLemmatizer()\n",
    "        for word, tag in pos_tag(entry):\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_final= word_lemmatized.lemmatize(word, part_of_speech_tag[tag[0]])\n",
    "                final_words.append(word_final)\n",
    "                df2.loc[index, 'text_final'] = str(final_words)\n",
    "\n",
    "\n",
    "    del df2['ColumnA']\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rd-jowxs_Nn_"
   },
   "source": [
    "Function to preprocess excel data (additional dataset to train the models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0VJYE4ftYdTn"
   },
   "outputs": [],
   "source": [
    "def preprocessinng2(x):\n",
    "    \n",
    "    df = pd.DataFrame(x)\n",
    "\n",
    "    df2 = pd.DataFrame(df['ColumnA'])\n",
    "    df2 = df2.astype(str)\n",
    "    df2.replace('#' , \"\", inplace= True, regex = True)\n",
    "    #df2.replace('_' ,\" \" , inplace= True, regex = True)\n",
    "    df2['ColumnA'] = [entry.lower () for entry in df2['ColumnA']]\n",
    "    df2.replace('angry','anger',inplace=True, regex = True)\n",
    "    df2.replace('furious','anger',inplace=True, regex = True)\n",
    "    df2.replace('irritated','anger',inplace=True, regex = True)\n",
    "    df2.replace('enraged','anger',inplace=True, regex = True)\n",
    "    df2.replace('annoyed','anger',inplace=True, regex = True)\n",
    "    df2.replace('sad','sadness',inplace=True, regex = True)\n",
    "    df2.replace('depressed','sadness',inplace=True, regex = True)\n",
    "    df2.replace('devastated','sadness',inplace=True, regex = True)\n",
    "    df2.replace('miserable','sadness',inplace=True, regex = True)\n",
    "    df2.replace('disappointed','sadness',inplace=True, regex = True)\n",
    "    df2.replace('terrified','fear',inplace=True, regex = True)\n",
    "    df2.replace('discouraged','fear',inplace=True, regex = True)\n",
    "    df2.replace('scared','fear',inplace=True, regex = True)\n",
    "    df2.replace('anxious','fear',inplace=True, regex = True)\n",
    "    df2.replace('fearful','fear',inplace=True, regex = True)\n",
    "    df2.replace('happy','joy',inplace=True, regex = True)\n",
    "    df2.replace('ecstatic','joy',inplace=True, regex = True)\n",
    "    df2.replace('glad','joy',inplace=True, regex = True)\n",
    "    df2.replace('relieved','joy',inplace=True, regex = True)\n",
    "    df2.replace('excited','joy',inplace=True, regex = True)\n",
    "    df2.replace('irritating','anger',inplace=True, regex = True)\n",
    "    df2.replace('vexing','anger',inplace=True, regex = True)\n",
    "    df2.replace('outrageous','anger',inplace=True, regex = True)\n",
    "    df2.replace('annoying','anger',inplace=True, regex = True)\n",
    "    df2.replace('displeasing','sadness',inplace=True, regex = True)\n",
    "    df2.replace('depressing','sadness',inplace=True, regex = True)\n",
    "    df2.replace('serious','sadness',inplace=True, regex = True)\n",
    "    df2.replace('grim','sadness',inplace=True, regex = True)\n",
    "    df2.replace('heartbreaking','sadness',inplace=True, regex = True)\n",
    "    df2.replace('gloomy','fear',inplace=True, regex = True)\n",
    "    df2.replace('horrible','fear',inplace=True, regex = True)\n",
    "    df2.replace('threatening','fear',inplace=True, regex = True)\n",
    "    df2.replace('terrifying','fear',inplace=True, regex = True)\n",
    "    df2.replace('shocking','fear',inplace=True, regex = True)\n",
    "    df2.replace('dreadful','joy',inplace=True, regex = True)\n",
    "    df2.replace('funny','joy',inplace=True, regex = True)\n",
    "    df2.replace('hilarious','joy',inplace=True, regex = True)\n",
    "    df2.replace('amazing','joy',inplace=True, regex = True)\n",
    "    df2.replace('wonderful','joy',inplace=True, regex = True)\n",
    "\n",
    "    # Remove blank rows.\n",
    "    df2['ColumnA'].dropna(inplace = True)\n",
    "\n",
    "    # Convert the text to lowercase\n",
    "    df2['ColumnA'] = [entry.lower () for entry in df2['ColumnA']]\n",
    "\n",
    "    # Tokenise the text \n",
    "    df2['ColumnA']= [word_tokenize (entry) for entry in df2['ColumnA']]\n",
    "\n",
    "    # Remove stop words and non alphabetic words and perform lemmatisation\n",
    "    for index, entry in enumerate(df2['ColumnA']):\n",
    "        final_words= []\n",
    "        word_lemmatized= WordNetLemmatizer()\n",
    "        for word, tag in pos_tag(entry):\n",
    "            if word not in stopwords.words('english') and word.isalpha():\n",
    "                word_final= word_lemmatized.lemmatize(word, part_of_speech_tag[tag[0]])\n",
    "                final_words.append(word_final)\n",
    "                df2.loc[index, 'text_final'] = str(final_words)\n",
    "\n",
    "\n",
    "    del df2['ColumnA']\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zxxKPvbjYf4y"
   },
   "outputs": [],
   "source": [
    "#preprocesing train dataset\n",
    "X_train_initial = preprocessinng(train_tweets_main)\n",
    "train_labels_df =     pd.DataFrame(train_labels_main)\n",
    "\n",
    "#preprocessing external dataset for training\n",
    "X_train_initial2 = preprocessinng2(df_train_tweet2_main)\n",
    "train_labels_df_2 = pd.DataFrame(df_train_label2_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WCR4h8dQesud"
   },
   "outputs": [],
   "source": [
    "#preprocessing public tweets\n",
    "public_tweets =preprocessinng(public_tweets)\n",
    "\n",
    "#preprocessing validation data\n",
    "Val_tweets = preprocessinng(Val_tweets_main)   \n",
    "val_labels = pd.DataFrame(Val_labels_main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rH7ANNYtepJZ"
   },
   "outputs": [],
   "source": [
    "#concatenating the tweets to split into test and train\n",
    "\n",
    "initial_tweet = pd.concat([X_train_initial,X_train_initial2])\n",
    "initial_labels = pd.concat([train_labels_df,train_labels_df_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R_Wbv-J1aTJb"
   },
   "source": [
    "**Conventional Machine Learning Model**\n",
    "\n",
    "The final model that produced the best-performing predictions for the Kaggle submission (accuracy 70.300%) was an SVM with a linear kernel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "_nfJhMzAaTJc",
    "outputId": "0abdac86-7263-4905-eb09-2e7a9e97ec71"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=50000,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words=None, strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df = pd.concat([initial_tweet, Val_tweets, public_tweets])\n",
    "combined_df = combined_df.astype(str)\n",
    "\n",
    "X_train_initial_combined = pd.concat([initial_tweet, Val_tweets])\n",
    "y_train = pd.concat([initial_labels, val_labels])\n",
    "X_test_initial = public_tweets\n",
    "\n",
    "#coverting the tweets to string datatype\n",
    "X_train_initial = X_train_initial_combined.astype(str)\n",
    "X_test_initial = X_test_initial.astype(str)\n",
    "\n",
    "#converting test set and train set to python series\n",
    "X_train = X_train_initial.text_final\n",
    "X_test = X_test_initial.text_final\n",
    "\n",
    "# Vectorize the words by using TF IDF Vectorizer\n",
    "tfidf_vect= TfidfVectorizer(max_features=50000)\n",
    "tfidf_vect.fit(combined_df['text_final'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y7U9Uu6_aTJg"
   },
   "outputs": [],
   "source": [
    "#inspecting the learned vocabulary \n",
    "print(tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsB8JcSeaTJl"
   },
   "outputs": [],
   "source": [
    "#Transform X_train and X_test to vectorized X_train_tfidf and X_test_tfidf\n",
    "X_train_tfidf= tfidf_vect.transform(X_train)\n",
    "X_test_tfidf = tfidf_vect.transform(X_test)\n",
    "\n",
    "#print(X_train_tfidf)\n",
    "#print(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRI84TjmJIpt"
   },
   "source": [
    "The model used here is a support vector classifier with a regularization parameter 1, a linear kernal and 'scale' as the kernal coefficiet. This was the optimum parameters for this model and rendered an accuracy of 70.300 % for public tweet dataset in kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "xxWO33FpaTJo",
    "outputId": "42acfaff-435d-445d-a29f-4268dcc8fad4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#SVM classifier#\n",
    "\n",
    "SVM = svm.SVC(C=1, kernel='linear', degree=3, gamma='scale')\n",
    "\n",
    "# Fit the training dataset.\n",
    "SVM.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict the labels on the validation dataset.\n",
    "predictions_SVM= SVM.predict(X_test_tfidf)\n",
    "\n",
    "#convert the prediction to DataFrame\n",
    "predictions_SVM = pd.DataFrame(predictions_SVM)\n",
    "\n",
    "#convert the DataFrame of predictions to csv format\n",
    "predictions_SVM.to_csv('45575657-conv.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SN4_T3JSFXwr"
   },
   "source": [
    "In addition to this model, I have also tried logistic regression model. The accuracy of this model (68.9%)  was a bit low when compared to the accuracy of SVC. This might be because SVM tries to finds the best margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data, while logistic regression does not, instead it can have different decision boundaries with different weights that are near the optimal point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mrS1gLuVaTJr"
   },
   "source": [
    "**DEEP LEARNING**\n",
    "\n",
    "The final model that produced the best-performing predictions for the Kaggle submission (accuracy 71.751%) was a Convolutional Neural network. Some further steps are required to input the preprocessed data to the deep learning model which are as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pGl5fID8aTJr"
   },
   "outputs": [],
   "source": [
    "#Converting the labels into categorical variables\n",
    "\n",
    "initial_labels = to_categorical(initial_labels)\n",
    "val_labels = to_categorical(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C_ERV77eiZcK"
   },
   "outputs": [],
   "source": [
    "#datatype conversion\n",
    "initial_tweet = initial_tweet.astype(str)\n",
    "Val_tweets = Val_tweets.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nnJ1-CTad4xz"
   },
   "outputs": [],
   "source": [
    "#convert the tweets to arrays\n",
    "\n",
    "sentences = initial_tweet['text_final'].values\n",
    "val_tweets = Val_tweets['text_final'].values   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4fL40Z0MP1cF"
   },
   "source": [
    "The CountVectorizer provided by the scikit-learn library is used to vectorize sentences. It takes the words of each sentence and creates a vocabulary of all the unique words in the sentences. This vocabulary can then be used to create a feature vector of the count of the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_J0NrjHpfNuG"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=100000)\n",
    "tokenizer.fit_on_texts(sentences) #confused if need to fit everything\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(sentences)\n",
    "X_test = tokenizer.texts_to_sequences(val_tweets)\n",
    "#val_tweets = tokenizer.texts_to_sequences(val_tweets)\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZ7ElUOzQmrk"
   },
   "source": [
    "One problem that we have is that each text sequence has in most cases different length of words. To counter this,  pad_sequence() is used which simply pads the sequence of words with zeros. By default, it prepends zeros.\n",
    "\n",
    "Additionally a maxlen parameter is added to specify how long the sequences should be. This cuts sequences that exceed that number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dJt5rjzoip_h"
   },
   "outputs": [],
   "source": [
    "#sentence padding\n",
    "maxlen = 250\n",
    "\n",
    "X_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n",
    "#val_tweets = pad_sequences(val_tweets, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lo5Vtw1YaTJ8"
   },
   "outputs": [],
   "source": [
    "#Preprocessing public tweets for deep learning model\n",
    "\n",
    "public_tweets = public_tweets.astype(str)\n",
    "public_tweets = public_tweets['text_final'].values  \n",
    "public_tweets = tokenizer.texts_to_sequences(public_tweets)\n",
    "public_tweets = pad_sequences(public_tweets, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xdnMNCd6RehI"
   },
   "source": [
    "An Embedding Layer of Keras which takes the previously calculated integers and maps them to a dense vector of the embedding is used here.\n",
    "\n",
    "The output of the embedding layer is taken and plugged it into a Convolutional layer. A pooling layer is added after this to downsample the input data. \n",
    "\n",
    "This is finally connected to a dense layer with 4 classes and a sigmoid activation.\n",
    "\n",
    "The model is then configured with categorical_crossentropy and categorical_accuracy loss function and metrics respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 329
    },
    "colab_type": "code",
    "id": "mH5LlPNhB8fi",
    "outputId": "42a28646-12ef-429a-f08c-2bfbb2555e93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 250, 100)          1867200   \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 246, 128)          64128     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 44        \n",
      "=================================================================\n",
      "Total params: 1,932,662\n",
      "Trainable params: 1,932,662\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\n",
    "model.add(layers.Conv1D(128, 5, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(10, activation='relu'))\n",
    "model.add(layers.Dense(4, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oAxKMsWmS4Qj"
   },
   "source": [
    "To train the model.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "GEMQJddDi8Or",
    "outputId": "25631e49-ce2e-42e3-9170-475274fd048f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, initial_labels,\n",
    "                    epochs=5,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_test, val_labels),\n",
    "                    batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "tZSg5TkoFVW6",
    "outputId": "7f8cea1d-bb12-43ae-83e2-69ba8e066fd1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9602\n",
      "Testing Accuracy:  0.7988\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_train, initial_labels, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(X_test, val_labels, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ErqD05kltxp"
   },
   "source": [
    "In addition to the final model, I also tried a CNN without the pooling layer. This has given me slightly lower accuracy (63.902%) when compared to the model with pooling layer. The pooling layer used here is maxpooling which take the maximum value of all features in the pool for each feature dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zJXizsS3rHw3"
   },
   "outputs": [],
   "source": [
    "tweets = model.predict(public_tweets , verbose=0)\n",
    "tweets_final = np.array([np.where(l == max(l), 1, 0) for l in tweets])\n",
    "out = np.argmax(tweets_final, axis = 1) \n",
    "out = pd.DataFrame(out)\n",
    "out.to_csv('45575657-deep.csv', index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZmWALUeikww-"
   },
   "source": [
    "Comparing my final conventional ML and deep learning models, the deep learning one performed better by **1.451%** on the public test set. The deep learning model performed well, with the top-performing system having 71.751% accuracy.\n",
    "\n",
    "Developer: **Midhun MJ**\n",
    "Date:  **July 23 2020**\n",
    "Place:  **Sydney, Australia**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ML Major Project 45575657 MIDHUN MURALIDHARAN JAYAKUMARI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
